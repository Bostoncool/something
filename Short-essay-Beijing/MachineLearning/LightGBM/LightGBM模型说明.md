# LightGBM PM2.5æµ“åº¦é¢„æµ‹æ¨¡å‹

## æ¨¡å‹ç®€ä»‹

LightGBM (Light Gradient Boosting Machine) æ˜¯å¾®è½¯å¼€å‘çš„é«˜æ•ˆæ¢¯åº¦æå‡å†³ç­–æ ‘æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

### ä¸»è¦ç‰¹ç‚¹

1. **è®­ç»ƒé€Ÿåº¦å¿«** - ä½¿ç”¨åŸºäºç›´æ–¹å›¾çš„ç®—æ³•ï¼Œæ¯”ä¼ ç»ŸGBDTå¿«10-20å€
2. **å†…å­˜æ¶ˆè€—ä½** - ç›´æ–¹å›¾ç®—æ³•å‡å°‘å†…å­˜å ç”¨
3. **å‡†ç¡®ç‡é«˜** - ä½¿ç”¨leaf-wiseç”Ÿé•¿ç­–ç•¥ï¼Œåœ¨ç›¸åŒè¿­ä»£æ¬¡æ•°ä¸‹é€šå¸¸æ¯”level-wiseå‡†ç¡®
4. **æ”¯æŒå¹¶è¡Œå­¦ä¹ ** - ç‰¹å¾å¹¶è¡Œã€æ•°æ®å¹¶è¡Œã€æŠ•ç¥¨å¹¶è¡Œ
5. **å¤„ç†å¤§è§„æ¨¡æ•°æ®** - å¯ä»¥å¤„ç†ç™¾ä¸‡çº§åˆ«çš„æ•°æ®å’Œç‰¹å¾
6. **æ”¯æŒç±»åˆ«ç‰¹å¾** - ç›´æ¥å¤„ç†ç±»åˆ«ç‰¹å¾ï¼Œæ— éœ€one-hotç¼–ç 

### ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | è®­ç»ƒé€Ÿåº¦ | é¢„æµ‹ç²¾åº¦ | å†…å­˜å ç”¨ | å¯è§£é‡Šæ€§ | å‚æ•°è°ƒä¼˜éš¾åº¦ |
|------|---------|---------|----------|----------|-------------|
| çº¿æ€§å›å½’(MLR) | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­ |
| éšæœºæ£®æ—(RF) | â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­ | â­â­ |
| XGBoost | â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­â­ |
| **LightGBM** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­ | â­â­â­ |

## æ–‡ä»¶è¯´æ˜

### ä¸»è¦æ–‡ä»¶

- `LightGBM_PM25.py` - å®Œæ•´ç‰ˆæœ¬ï¼Œä½¿ç”¨çœŸå®æ•°æ®è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹
- `LightGBM_PM25_Simple.py` - ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å¿«é€Ÿæµ‹è¯•
- `LightGBMæ¨¡å‹è¯´æ˜.md` - æœ¬è¯´æ˜æ–‡æ¡£

### è¾“å‡ºæ–‡ä»¶

è¿è¡Œåä¼šåœ¨ä»¥ä¸‹ç›®å½•ç”Ÿæˆç»“æœï¼š

#### output/ ç›®å½•
- `model_performance.csv` - æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
- `feature_importance.csv` - ç‰¹å¾é‡è¦æ€§
- `best_parameters.csv` - æœ€ä½³è¶…å‚æ•°
- `predictions.csv` - é¢„æµ‹ç»“æœ
- `*.png` - å„ç§å¯è§†åŒ–å›¾è¡¨

#### models/ ç›®å½•
- `lightgbm_optimized.txt` - æ¨¡å‹æ–‡ä»¶ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
- `lightgbm_optimized.pkl` - æ¨¡å‹æ–‡ä»¶ï¼ˆpickleæ ¼å¼ï¼‰

## ç¯å¢ƒé…ç½®

### å¿…éœ€åº“

```bash
pip install pandas numpy matplotlib seaborn scikit-learn lightgbm
```

### å¯é€‰åº“ï¼ˆç”¨äºè´å¶æ–¯ä¼˜åŒ–ï¼‰

```bash
pip install bayesian-optimization
```

## æ•°æ®è¦æ±‚

### è¾“å…¥æ•°æ®

1. **æ±¡æŸ“æ•°æ®**
   - è·¯å¾„: `C:\Users\IU\Desktop\Datebase Origin\Benchmark\`
   - æ ¼å¼: CSVæ–‡ä»¶ï¼ŒæŒ‰æ—¥æœŸå‘½å (YYYYMMDD)
   - å˜é‡: PM2.5, PM10, SO2, NO2, CO, O3

2. **æ°”è±¡æ•°æ®**
   - è·¯å¾„: `C:\Users\IU\Desktop\Datebase Origin\ERA5-Beijing-CSV\`
   - æ ¼å¼: CSVæ–‡ä»¶ï¼ŒæŒ‰å¹´æœˆå‘½å (YYYYMM.csv)
   - å˜é‡: t2m, d2m, u10, v10, u100, v100, blh, sp, tcwv, tpç­‰

### ç‰¹å¾å·¥ç¨‹

æ¨¡å‹è‡ªåŠ¨åˆ›å»ºä»¥ä¸‹ç‰¹å¾ï¼š

1. **é£é€Ÿç‰¹å¾**
   - 10ç±³é£é€Ÿå’Œé£å‘
   - 100ç±³é£é€Ÿå’Œé£å‘

2. **æ—¶é—´ç‰¹å¾**
   - å¹´ã€æœˆã€æ—¥ã€æ˜ŸæœŸå‡ 
   - ä¸€å¹´ä¸­çš„ç¬¬å‡ å¤©ã€ç¬¬å‡ å‘¨
   - å­£èŠ‚ã€æ˜¯å¦ä¾›æš–å­£

3. **æ»åç‰¹å¾**
   - PM2.5çš„1å¤©ã€3å¤©ã€7å¤©æ»åå€¼
   - PM2.5çš„3å¤©ã€7å¤©ã€30å¤©ç§»åŠ¨å¹³å‡

4. **è¡ç”Ÿç‰¹å¾**
   - æ¸©åº¦-éœ²ç‚¹å·®ï¼ˆç›¸å¯¹æ¹¿åº¦æŒ‡æ ‡ï¼‰
   - ç›¸å¯¹æ¹¿åº¦ä¼°ç®—
   - é£å‘åˆ†ç±»ï¼ˆ8ä¸ªæ–¹ä½ï¼‰

## ä½¿ç”¨æ–¹æ³•

### 1. å¿«é€Ÿå¼€å§‹ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰

```bash
python LightGBM_PM25_Simple.py
```

é€‚ç”¨åœºæ™¯ï¼š
- å¿«é€Ÿæµ‹è¯•ä»£ç é€»è¾‘
- äº†è§£æ¨¡å‹å·¥ä½œåŸç†
- æ— éœ€ç­‰å¾…é•¿æ—¶é—´æ•°æ®åŠ è½½

### 2. å®Œæ•´è®­ç»ƒï¼ˆçœŸå®æ•°æ®ï¼‰

```bash
python LightGBM_PM25.py
```

è¿è¡Œæ—¶é—´ï¼š
- æ•°æ®åŠ è½½: 5-10åˆ†é’Ÿ
- æ¨¡å‹è®­ç»ƒ: 2-5åˆ†é’Ÿ
- æ€»è®¡: çº¦10-15åˆ†é’Ÿ

## æ¨¡å‹å‚æ•°

### æ ¸å¿ƒå‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | è°ƒä¼˜èŒƒå›´ |
|------|------|--------|---------|
| `num_leaves` | å¶å­èŠ‚ç‚¹æ•° | 31 | 20-100 |
| `max_depth` | æ ‘çš„æœ€å¤§æ·±åº¦ | 7 | 3-12 |
| `learning_rate` | å­¦ä¹ ç‡ | 0.05 | 0.01-0.1 |
| `feature_fraction` | ç‰¹å¾é‡‡æ ·æ¯”ä¾‹ | 0.8 | 0.5-1.0 |
| `bagging_fraction` | æ ·æœ¬é‡‡æ ·æ¯”ä¾‹ | 0.8 | 0.5-1.0 |
| `min_child_samples` | å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•° | 20 | 10-50 |

### å‚æ•°è°ƒä¼˜å»ºè®®

1. **é˜²æ­¢è¿‡æ‹Ÿåˆ**
   - é™ä½ `num_leaves` å’Œ `max_depth`
   - å¢åŠ  `min_child_samples`
   - é™ä½ `learning_rate`ï¼Œå¢åŠ  `num_boost_round`

2. **æé«˜ç²¾åº¦**
   - å¢åŠ  `num_boost_round`
   - è°ƒæ•´ `feature_fraction` å’Œ `bagging_fraction`
   - ä½¿ç”¨æ—©åœæœºåˆ¶

3. **åŠ å¿«è®­ç»ƒ**
   - å‡å°‘ `num_boost_round`
   - å¢åŠ  `learning_rate`
   - é™ä½ `max_depth`

## è¶…å‚æ•°ä¼˜åŒ–

### è´å¶æ–¯ä¼˜åŒ–ï¼ˆæ¨èï¼‰

å¦‚æœå®‰è£…äº† `bayesian-optimization`ï¼Œä»£ç ä¼šè‡ªåŠ¨ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼š

```python
# è‡ªåŠ¨æœç´¢æœ€ä½³å‚æ•°
optimizer.maximize(init_points=5, n_iter=15)
```

ä¼˜åŠ¿ï¼š
- æ™ºèƒ½æœç´¢ï¼Œæ•ˆç‡é«˜
- éœ€è¦æ›´å°‘çš„è¿­ä»£æ¬¡æ•°
- å¯ä»¥æ‰¾åˆ°æ›´å¥½çš„å‚æ•°ç»„åˆ

### ç½‘æ ¼æœç´¢ï¼ˆå¤‡é€‰ï¼‰

å¦‚æœæœªå®‰è£…è´å¶æ–¯ä¼˜åŒ–åº“ï¼Œä½¿ç”¨ç½‘æ ¼æœç´¢ï¼š

```python
param_grid = {
    'num_leaves': [31, 50, 70],
    'max_depth': [5, 7, 9],
    'learning_rate': [0.03, 0.05, 0.07],
    'feature_fraction': [0.7, 0.8, 0.9],
}
```

## æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

### 1. RÂ² (å†³å®šç³»æ•°)
- èŒƒå›´: 0-1ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
- å«ä¹‰: æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹
- ä¼˜ç§€: > 0.8, è‰¯å¥½: 0.6-0.8, ä¸€èˆ¬: 0.4-0.6

### 2. RMSE (å‡æ–¹æ ¹è¯¯å·®)
- å•ä½: Î¼g/mÂ³ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
- å«ä¹‰: é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å¹³å‡åå·®
- ä¼˜ç§€: < 15, è‰¯å¥½: 15-25, ä¸€èˆ¬: 25-40

### 3. MAE (å¹³å‡ç»å¯¹è¯¯å·®)
- å•ä½: Î¼g/mÂ³ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
- å«ä¹‰: é¢„æµ‹è¯¯å·®çš„å¹³å‡ç»å¯¹å€¼
- å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ

### 4. MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®)
- å•ä½: %ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
- å«ä¹‰: ç›¸å¯¹è¯¯å·®çš„ç™¾åˆ†æ¯”
- ä¼˜ç§€: < 15%, è‰¯å¥½: 15-25%, ä¸€èˆ¬: 25-40%

## ç‰¹å¾é‡è¦æ€§

### ä¸¤ç§é‡è¦æ€§åº¦é‡

1. **Splitï¼ˆåˆ†è£‚æ¬¡æ•°ï¼‰**
   - ç‰¹å¾åœ¨æ ‘ä¸­è¢«ç”¨ä½œåˆ†è£‚çš„æ¬¡æ•°
   - åæ˜ ç‰¹å¾ä½¿ç”¨é¢‘ç‡

2. **Gainï¼ˆä¿¡æ¯å¢ç›Šï¼‰**
   - ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½çš„å®é™…è´¡çŒ®
   - æ›´èƒ½åæ˜ ç‰¹å¾çš„çœŸå®é‡è¦æ€§ï¼ˆæ¨èï¼‰

### é¢„æœŸé‡è¦ç‰¹å¾

æ ¹æ®PM2.5å½¢æˆæœºç†ï¼Œé€šå¸¸æœ€é‡è¦çš„ç‰¹å¾åŒ…æ‹¬ï¼š

1. **æ»åç‰¹å¾** - PM2.5_lag1, PM2.5_lag3, PM2.5_ma3
2. **æ¸©åº¦** - t2m, d2m, temp_dewpoint_diff
3. **é£é€Ÿ** - wind_speed_10m, u10, v10
4. **æ¹¿åº¦** - tcwv, relative_humidity
5. **è¾¹ç•Œå±‚é«˜åº¦** - blh
6. **æ—¶é—´ç‰¹å¾** - month, season, is_heating_season

## å¯è§†åŒ–å›¾è¡¨

### 1. è®­ç»ƒè¿‡ç¨‹æ›²çº¿
- å±•ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„RMSEå˜åŒ–
- å¸®åŠ©åˆ¤æ–­æ˜¯å¦è¿‡æ‹Ÿåˆ
- æ˜¾ç¤ºæœ€ä½³è¿­ä»£æ¬¡æ•°

### 2. é¢„æµ‹vså®é™…æ•£ç‚¹å›¾
- å¯¹æ¯”é¢„æµ‹å€¼å’ŒçœŸå®å€¼
- è¶Šæ¥è¿‘å¯¹è§’çº¿è¡¨ç¤ºé¢„æµ‹è¶Šå‡†ç¡®
- åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†

### 3. æ—¶é—´åºåˆ—å¯¹æ¯”
- å±•ç¤ºé¢„æµ‹æ›²çº¿å’Œå®é™…æ›²çº¿
- ç›´è§‚å±•ç¤ºæ¨¡å‹è·Ÿè¸ªèƒ½åŠ›
- æ˜¾ç¤ºæœ€å300å¤©çš„é¢„æµ‹æ•ˆæœ

### 4. æ®‹å·®åˆ†æ
- æ®‹å·®åº”éšæœºåˆ†å¸ƒåœ¨0é™„è¿‘
- å¦‚æœæœ‰æ˜æ˜¾æ¨¡å¼ï¼Œè¯´æ˜æ¨¡å‹è¿˜å¯æ”¹è¿›
- å¸®åŠ©è¯†åˆ«ç³»ç»Ÿæ€§åå·®

### 5. ç‰¹å¾é‡è¦æ€§å›¾
- å±•ç¤ºTop 20é‡è¦ç‰¹å¾
- æŒ‰Splitå’ŒGainä¸¤ç§æ–¹å¼æ’åº
- æŒ‡å¯¼ç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹

### 6. æ¨¡å‹æ€§èƒ½å¯¹æ¯”
- å¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹
- å±•ç¤ºRÂ²ã€RMSEã€MAEã€MAPE
- é‡åŒ–ä¼˜åŒ–æ•ˆæœ

### 7. è¯¯å·®åˆ†å¸ƒ
- å±•ç¤ºé¢„æµ‹è¯¯å·®çš„åˆ†å¸ƒ
- åº”å‘ˆæ­£æ€åˆ†å¸ƒ
- å‡å€¼åº”æ¥è¿‘0

## æ¨¡å‹ä½¿ç”¨

### åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹

```python
import lightgbm as lgb
import pickle

# æ–¹æ³•1: åŠ è½½æ–‡æœ¬æ ¼å¼
model = lgb.Booster(model_file='models/lightgbm_optimized.txt')

# æ–¹æ³•2: åŠ è½½pickleæ ¼å¼
with open('models/lightgbm_optimized.pkl', 'rb') as f:
    model = pickle.load(f)
```

### è¿›è¡Œé¢„æµ‹

```python
# å‡†å¤‡æ–°æ•°æ®ï¼ˆç‰¹å¾å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
X_new = prepare_new_data()  # ä½ çš„æ•°æ®å‡†å¤‡å‡½æ•°

# é¢„æµ‹
predictions = model.predict(X_new, num_iteration=model.best_iteration)

# é¢„æµ‹ç»“æœä¸ºPM2.5æµ“åº¦ï¼ˆÎ¼g/mÂ³ï¼‰
print(f"é¢„æµ‹çš„PM2.5æµ“åº¦: {predictions}")
```

## å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆéœ€è¦éªŒè¯é›†ï¼Ÿ

A: éªŒè¯é›†ç”¨äºï¼š
- æ—©åœæœºåˆ¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- è¶…å‚æ•°è°ƒä¼˜
- ç›‘æ§è®­ç»ƒè¿‡ç¨‹

### Q2: å¦‚ä½•å¤„ç†æ•°æ®ä¸è¶³çš„æƒ…å†µï¼Ÿ

A: å¯ä»¥ï¼š
- å‡å°‘éªŒè¯é›†æ¯”ä¾‹
- ä½¿ç”¨äº¤å‰éªŒè¯
- å‡å°‘æ»åç‰¹å¾ï¼ˆå‡å°‘æ ·æœ¬æŸå¤±ï¼‰

### Q3: è®­ç»ƒæ—¶é—´å¤ªé•¿æ€ä¹ˆåŠï¼Ÿ

A: å¯ä»¥ï¼š
- å‡å°‘ `num_boost_round`
- å¢åŠ  `learning_rate`
- å‡å°‘æ•°æ®é‡ï¼ˆæŒ‰æœˆæˆ–å­£åº¦é‡‡æ ·ï¼‰
- å‡å°‘è¶…å‚æ•°æœç´¢ç©ºé—´

### Q4: å¦‚ä½•æé«˜æ¨¡å‹ç²¾åº¦ï¼Ÿ

A: å¯ä»¥å°è¯•ï¼š
- å¢åŠ æ›´å¤šç‰¹å¾ï¼ˆå¦‚å…¶ä»–æ±¡æŸ“ç‰©ï¼‰
- è°ƒæ•´è¶…å‚æ•°
- å¢åŠ è®­ç»ƒæ•°æ®
- ç‰¹å¾å·¥ç¨‹ï¼ˆäº¤äº’ç‰¹å¾ã€å¤šé¡¹å¼ç‰¹å¾ï¼‰
- é›†æˆå¤šä¸ªæ¨¡å‹

### Q5: æ¨¡å‹åœ¨æŸäº›æ—¶æ®µé¢„æµ‹ä¸å‡†ï¼Ÿ

A: å¯èƒ½åŸå› ï¼š
- æç«¯å¤©æ°”äº‹ä»¶ï¼ˆæ•°æ®åˆ†å¸ƒå¤–ï¼‰
- äººä¸ºæ’æ”¾å˜åŒ–ï¼ˆå¦‚æ”¿ç­–å½±å“ï¼‰
- ç‰¹æ®Šæ—¶æœŸï¼ˆå¦‚æ˜¥èŠ‚ã€é‡å¤§æ´»åŠ¨ï¼‰
- å»ºè®®ï¼šæ·»åŠ äº‹ä»¶æ ‡è®°ç‰¹å¾

## è¿›é˜¶åº”ç”¨

### 1. å¤šæ­¥é¢„æµ‹

```python
# é¢„æµ‹æœªæ¥7å¤©
predictions_7day = []
X_current = X_test.iloc[-1:].copy()

for i in range(7):
    pred = model.predict(X_current)
    predictions_7day.append(pred[0])
    
    # æ›´æ–°æ»åç‰¹å¾
    X_current['PM2.5_lag1'] = pred[0]
    # ... æ›´æ–°å…¶ä»–ç‰¹å¾
```

### 2. ä¸ç¡®å®šæ€§ä¼°è®¡

```python
# ä½¿ç”¨åˆ†ä½æ•°å›å½’
model_lower = lgb.train(params, train_data, objective='quantile', alpha=0.1)
model_upper = lgb.train(params, train_data, objective='quantile', alpha=0.9)

# é¢„æµ‹åŒºé—´
pred_lower = model_lower.predict(X_test)
pred_upper = model_upper.predict(X_test)
```

### 3. ç‰¹å¾é€‰æ‹©

```python
# åŸºäºé‡è¦æ€§ç­›é€‰ç‰¹å¾
importance_threshold = 1.0  # 1%
important_features = feature_importance[
    feature_importance['Importance_Gain_Norm'] > importance_threshold
]['Feature'].tolist()

# é‡æ–°è®­ç»ƒ
X_train_selected = X_train[important_features]
```

## å‚è€ƒèµ„æ–™

### LightGBMå®˜æ–¹æ–‡æ¡£
- GitHub: https://github.com/microsoft/LightGBM
- æ–‡æ¡£: https://lightgbm.readthedocs.io/

### ç›¸å…³è®ºæ–‡
1. Ke et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
2. Chen & Guestrin (2016). "XGBoost: A Scalable Tree Boosting System"

### æ¨èé˜…è¯»
- LightGBMè°ƒå‚æŒ‡å—
- æ¢¯åº¦æå‡å†³ç­–æ ‘åŸç†
- æ—¶é—´åºåˆ—ç‰¹å¾å·¥ç¨‹

## æ›´æ–°æ—¥å¿—

### v1.0 (2024)
- åˆå§‹ç‰ˆæœ¬
- æ”¯æŒåŸºç¡€LightGBMè®­ç»ƒ
- è´å¶æ–¯ä¼˜åŒ–è¶…å‚æ•°
- å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹
- ä¸°å¯Œçš„å¯è§†åŒ–

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- åœ¨ä»£ç ä»“åº“æIssue
- æŸ¥çœ‹ç›¸å…³æ–‡æ¡£å’Œç¤ºä¾‹

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼** ğŸš€

