# GPUä½¿ç”¨ç‡ä½çš„åŸå› åˆ†æä¸è§£å†³æ–¹æ¡ˆ

## ä¸€ã€é—®é¢˜è¯Šæ–­

### 1.1 å½“å‰é…ç½®åˆ†æ
- **Batch Size**: 128ï¼ˆå¯èƒ½åå°ï¼‰
- **æ¨¡å‹å¤§å°**: hidden_size=64, num_layers=2ï¼ˆè®¡ç®—é‡ä¸è¶³ï¼‰
- **DataLoader**: num_workers=4ï¼ˆå¯èƒ½ä¸å¤Ÿï¼‰
- **åºåˆ—é•¿åº¦**: 7-30å¤©ï¼ˆç›¸å¯¹è¾ƒçŸ­ï¼‰
- **æ··åˆç²¾åº¦**: å·²å¯ç”¨ âœ“
- **Pin Memory**: å·²å¯ç”¨ âœ“

### 1.2 ä¸»è¦ç“¶é¢ˆè¯†åˆ«

#### ğŸ”´ ä¸¥é‡é—®é¢˜
1. **Batch Sizeå¤ªå°**: 128å¯¹äºç°ä»£GPUï¼ˆå¦‚V100/A100ï¼‰æ¥è¯´å¤ªå°ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›
2. **æ¨¡å‹è®¡ç®—é‡ä¸è¶³**: hidden_size=64å¤ªå°ï¼ŒLSTMçš„è®¡ç®—å¯†åº¦ä½ï¼ŒGPUåˆ©ç”¨ç‡ä½
3. **ç¼ºå°‘æ•°æ®é¢„å–**: DataLoaderæ²¡æœ‰è®¾ç½®prefetch_factorï¼ŒGPUç»å¸¸ç­‰å¾…æ•°æ®
4. **CPU-GPUåŒæ­¥é˜»å¡**: evaluate_modelä¸­é¢‘ç¹çš„`.cpu().numpy()`å¯¼è‡´åŒæ­¥ç­‰å¾…

#### ğŸŸ¡ ä¸­ç­‰é—®é¢˜
5. **æ²¡æœ‰ä½¿ç”¨torch.compile**: PyTorch 2.0+çš„ç¼–è¯‘ä¼˜åŒ–å¯ä»¥æ˜¾è‘—æå‡æ€§èƒ½
6. **optimizer.zero_grad()æ•ˆç‡ä½**: åº”è¯¥ä½¿ç”¨`set_to_none=True`
7. **ç¼ºå°‘æ¢¯åº¦ç´¯ç§¯**: æ— æ³•æ¨¡æ‹Ÿæ›´å¤§çš„batch size
8. **NUM_WORKERSå¯èƒ½ä¸å¤Ÿ**: 4ä¸ªworkerå¯èƒ½æ— æ³•åŠæ—¶æä¾›æ•°æ®

#### ğŸŸ¢ è½»å¾®é—®é¢˜
9. **åºåˆ—é•¿åº¦è¾ƒçŸ­**: 7-30å¤©çš„åºåˆ—å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨GPUçš„å¹¶è¡Œèƒ½åŠ›
10. **æ²¡æœ‰ä½¿ç”¨torch.backends.cudnn.allow_tf32**: TensorFloat-32å¯ä»¥æå‡æ€§èƒ½

## äºŒã€ä¼˜åŒ–æ–¹æ¡ˆ

### 2.1 ç«‹å³ä¼˜åŒ–ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

#### ä¼˜åŒ–1: å¢å¤§Batch Size
```python
BATCH_SIZE = 512  # ä»128å¢åŠ åˆ°512ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰
```

#### ä¼˜åŒ–2: å¢å¤§æ¨¡å‹è§„æ¨¡
```python
basic_params = {
    'hidden_size': 256,  # ä»64å¢åŠ åˆ°256
    'num_layers': 3,     # ä»2å¢åŠ åˆ°3
    ...
}
```

#### ä¼˜åŒ–3: ä¼˜åŒ–DataLoaderé…ç½®
```python
train_loader = DataLoader(
    train_dataset, 
    batch_size=BATCH_SIZE, 
    shuffle=False,
    num_workers=8,  # å¢åŠ åˆ°8
    pin_memory=True,
    persistent_workers=True,
    prefetch_factor=4,  # æ–°å¢ï¼šé¢„å–4ä¸ªbatch
    drop_last=False
)
```

#### ä¼˜åŒ–4: ç§»é™¤ä¸å¿…è¦çš„CPU-GPUåŒæ­¥
```python
# åœ¨evaluate_modelä¸­ï¼Œå»¶è¿Ÿè½¬æ¢åˆ°CPU
predictions = []
with torch.no_grad():
    for X_batch, _ in data_loader:
        X_batch = X_batch.to(device, non_blocking=True)
        with autocast(enabled=USE_AMP):
            outputs = model(X_batch)
        predictions.append(outputs)  # ä¿æŒåœ¨GPUä¸Š
        
# æœ€åä¸€æ¬¡æ€§è½¬æ¢
predictions = torch.cat(predictions).cpu().numpy()
```

#### ä¼˜åŒ–5: ä½¿ç”¨torch.compileï¼ˆPyTorch 2.0+ï¼‰
```python
if hasattr(torch, 'compile'):
    model = torch.compile(model, mode='max-autotune')
```

#### ä¼˜åŒ–6: ä¼˜åŒ–optimizer.zero_grad()
```python
optimizer.zero_grad(set_to_none=True)  # æ›´é«˜æ•ˆ
```

### 2.2 è¿›é˜¶ä¼˜åŒ–ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

#### ä¼˜åŒ–7: å¯ç”¨TensorFloat-32
```python
torch.backends.cudnn.allow_tf32 = True
torch.backends.cuda.matmul.allow_tf32 = True
```

#### ä¼˜åŒ–8: ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆå¦‚æœå†…å­˜ä¸è¶³ï¼‰
```python
accumulation_steps = 4  # ç´¯ç§¯4ä¸ªbatchçš„æ¢¯åº¦
effective_batch_size = BATCH_SIZE * accumulation_steps
```

#### ä¼˜åŒ–9: å¢åŠ åºåˆ—é•¿åº¦
```python
SEQUENCE_LENGTHS = [30, 60, 90]  # å¢åŠ åºåˆ—é•¿åº¦ä»¥æå‡è®¡ç®—å¯†åº¦
```

### 2.3 ç›‘æ§ä¸è°ƒè¯•

#### æ·»åŠ GPUåˆ©ç”¨ç‡ç›‘æ§
```python
import subprocess
def get_gpu_utilization():
    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                          capture_output=True, text=True)
    return int(result.stdout.strip())
```

## ä¸‰ã€é¢„æœŸæ•ˆæœ

### ä¼˜åŒ–å‰
- GPUåˆ©ç”¨ç‡: ~20-40%
- è®­ç»ƒé€Ÿåº¦: ~500-1000 samples/s
- Batchå¤„ç†æ—¶é—´: ~50-100ms/batch

### ä¼˜åŒ–åï¼ˆé¢„æœŸï¼‰
- GPUåˆ©ç”¨ç‡: ~80-95%
- è®­ç»ƒé€Ÿåº¦: ~3000-5000 samples/sï¼ˆæå‡3-5å€ï¼‰
- Batchå¤„ç†æ—¶é—´: ~10-20ms/batchï¼ˆæå‡5å€ï¼‰

## å››ã€å®æ–½å»ºè®®

1. **é€æ­¥ä¼˜åŒ–**: å…ˆå®æ–½é«˜ä¼˜å…ˆçº§ä¼˜åŒ–ï¼Œè§‚å¯Ÿæ•ˆæœåå†è¿›è¡Œè¿›é˜¶ä¼˜åŒ–
2. **ç›‘æ§GPU**: ä½¿ç”¨`nvidia-smi -l 1`å®æ—¶ç›‘æ§GPUåˆ©ç”¨ç‡
3. **å†…å­˜ç®¡ç†**: å¢å¤§batch sizeæ—¶æ³¨æ„GPUå†…å­˜é™åˆ¶
4. **æ€§èƒ½æµ‹è¯•**: æ¯æ¬¡ä¼˜åŒ–åè®°å½•è®­ç»ƒé€Ÿåº¦ï¼Œå¯¹æ¯”æ•ˆæœ

